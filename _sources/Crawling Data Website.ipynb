{"cells":[{"cell_type":"markdown","source":["# **Crawling Berita Online**"],"metadata":{"id":"oh_0lWrrLaiD"}},{"cell_type":"markdown","source":["## Konsep Crawling\n","\n","Crawling data, atau web scraping, adalah proses otomatis untuk mengumpulkan data dari situs web menggunakan program yang disebut web crawler atau scraper. Program ini bekerja dengan mengakses halaman-halaman web, membaca konten HTML, dan mengekstrak informasi yang relevan seperti teks, gambar, atau tautan.\n","\n","Untuk melakukan ini, crawler mengirimkan permintaan HTTP ke server web dan kemudian menerima konten halaman sebagai respons. Setelah konten halaman diperoleh, teknik parsing digunakan untuk menavigasi struktur HTML dan mengambil data yang diinginkan, seringkali dengan bantuan pustaka seperti BeautifulSoup atau Scrapy. Data yang diambil kemudian disimpan dalam format yang terstruktur, seperti CSV, JSON, atau dalam database, agar mudah dianalisis lebih lanjut. Saat melakukan crawling, penting untuk memperhatikan frekuensi permintaan agar tidak membebani server dan tetap mematuhi aturan yang ada, seperti file robots.txt atau ketentuan layanan situs web. Web scraping digunakan dalam berbagai aplikasi, termasuk pengumpulan data berita, pemantauan harga produk, dan analisis media sosial. Meskipun sangat berguna, kegiatan ini harus dilakukan dengan mematuhi etika dan peraturan hukum yang berlaku."],"metadata":{"id":"y70F3pIsNAMu"}},{"cell_type":"markdown","source":["## Teknik dan Cara Crawling Menggunakan Python\n","Crawling data menggunakan Python dapat dilakukan dengan berbagai cara, mulai dari menggunakan pustaka sederhana seperti Requests dan BeautifulSoup untuk kebutuhan dasar, hingga framework yang lebih kompleks seperti Scrapy untuk kebutuhan yang lebih lanjut dan skalabilitas yang lebih tinggi. Teknik ini memungkinkan pengumpulan data otomatis dari situs web untuk berbagai tujuan, seperti analisis data, penelitian, dan pemantauan informasi secara real-time.\n","\n","Berikut merupakan teknik dan cara Crawling menggunakan Python:\n","1. Menggunakan Requests dan BeautifulSoup\n","  Berikut merupakan langkah menggunakan Request dan BeautifulSoup untuk Crawling:\n","  1. Install library yang dibutuhkan, pastikan untuk menginstal library requests dan beautifulsoup4.\n","  2. Mnegirim permintaan HTTP dan mendapatkan konten halaman, Gunakan requests untuk mengirim permintaan HTTP ke halaman web yang ingin di-crawl.\n","  3. Parsing HTML dengan BeautifulSoup, gunakan BeautifulSoup untuk parsing HTML dan mengekstrak data yang dibutuhkan.\n","  4. Menyimpan Data yang Diperoleh setelah mengekstraksi data, simpan dalam format yang diinginkan seperti CSV, JSON, atau database.\n","\n","2. Menggunakan Scrapy\n","\n","  Scrapy adalah framework crawling dan scraping yang lebih kuat dan canggih, yang memungkinkan scraping data dari beberapa halaman dengan efisiensi tinggi. Scrapy juga menyediakan berbagai fitur seperti manajemen sesi, pemrosesan asinkron, dan dukungan untuk scraping paralel.\n","\n","  Berikut merupakan langkah-langkah menggunakan scrapy:\n","  1. Install scrapy\n","  2. Buat proyek scrapy baru\n","  3. Membuat spider scrapy\n","  4. Menulis logika crawling dalam spider, buka file spider lalu tulis logika untuk merayapi dan mengekstrak data yang diinginkan.\n","  5. Jalankan spider"],"metadata":{"id":"OZM0HoECOOmt"}},{"cell_type":"markdown","source":["## Tool yang Digunakan (Library)\n","\n","1. Request\n","  \n","     Pustaka ini digunakan untuk mengirim permintaan HTTP ke server web dan mendapatkan konten halaman web. Permintaan HTTP dapat menggunakan metode GET (untuk mengambil data) atau POST (untuk mengirim data).\n","2. BeautifulSoup\n","      Setelah konten halaman diperoleh menggunakan Requests, BeautifulSoup digunakan untuk parsing HTML dan mengekstrak elemen-elemen yang diinginkan berdasarkan tag HTML, atribut, atau struktur lainnya.\n","3. Pandas\n","\n","      Pustaka Python open source yang berfungsi untuk mengolah dan menganalisis data, disini pandas akan digunakan untuk mengelola data hasil scraping dalam bentuk tabel (DataFrame) dan memanipulasi data lebih lanjut.\n","\n","4. Time\n","\n","      Untuk mengatur jeda antara permintaan saat melakukan scraping agar tidak terdeteksi sebagai bot oleh server target."],"metadata":{"id":"Z23xFy6bbBm1"}},{"cell_type":"markdown","source":["## Proses Crawling"],"metadata":{"id":"1FCYqnMgd0p-"}},{"cell_type":"markdown","source":["Install pustaka yang dibutuhkan"],"metadata":{"id":"P9y7_Ywzd8Ky"}},{"cell_type":"code","source":["!pip install requests\n","!pip install beautifulsoup4\n","!pip install pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qF_T6fvpavJ-","executionInfo":{"status":"ok","timestamp":1725459827573,"user_tz":-420,"elapsed":10561,"user":{"displayName":"Firdatul A'yuni","userId":"04335010483596107898"}},"outputId":"3ee64489-053a-435e-8742-e73daa2b18d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.20)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}]},{"cell_type":"markdown","source":["**Mengatur URL Dasar**\n","\n","Isi dengan URL dasar yang akan digunakan untuk mengakses halaman kategori \"kuliner\" di situs web Kompas."],"metadata":{"id":"fPxzJ8nWdW0X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2WLOf8EIdrI"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","# URL dasar untuk kategori 'kuliner'\n","base_url = 'https://www.kompas.id/kategori/gaya-hidup/kuliner?open_from=Side_Menu'\n"]},{"cell_type":"markdown","source":["**Inisiliasi List Untuk Menyimpan Hasil Scrape**\n","\n","Membuat list kosong yang akan diisi dengan data berita yang berhasil di-scrape dari situs web."],"metadata":{"id":"D0lDVTykeaN5"}},{"cell_type":"code","source":["# Inisialisasi list untuk menyimpan hasil scrape\n","data_berita = []"],"metadata":{"id":"JXpKTt5jdtBR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Menambahkan Headers**\n","\n","Menentukan \"User-Agent\" agar permintaan terlihat seperti berasal dari browser biasa, untuk menghindari blokir dari situs web yang mendeteksi permintaan otomatis."],"metadata":{"id":"hK0MoP6meoG-"}},{"cell_type":"code","source":["# Tambahkan headers\n","headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n","}"],"metadata":{"id":"z3B6TOcAdv80"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Scraping**\n","\n","1. Loop pertama yaitu Loop untuk Pagination\n","\n","  - Loop ini digunakan untuk mengakses beberapa halaman (dalam hal ini, halaman 1 hingga 10) dari kategori \"kuliner\" di situs Kompas. Angka 11 di range(1, 11) menunjukkan bahwa loop ini berjalan untuk 10 halaman (1 sampai 10).\n","  - Menggunakan f-string untuk membuat URL lengkap untuk setiap halaman. base_url adalah URL dasar dari kategori \"kuliner\", dan page={page} menambahkan nomor halaman yang sedang diakses.\n","  - Mengirim permintaan HTTP GET ke URL yang sudah dihasilkan dengan requests.get(). headers berisi informasi \"User-Agent\" untuk menyamar sebagai browser.\n","  - Menggunakan BeautifulSoup untuk mem-parsing respons HTML yang diterima dari situs web, agar kita bisa mengekstrak data yang diperlukan.\n","2. Tunggu Sebelum Permintaan Berikutnya\n","  Membuat jeda waktu selama 3 detik sebelum mengirim permintaan berikutnya. Ini bertujuan untuk menghindari pengiriman terlalu banyak permintaan dalam waktu singkat yang bisa menyebabkan server menolak permintaan lebih lanjut atau memblokir IP.\n","3. Menggunakan find_all untuk mencari semua elemen <div> dengan kelas CSS tertentu yang mengidentifikasi bahwa elemen tersebut berisi informasi berita. Kelas CSS digunakan untuk menemukan bagian yang relevan dari halaman web.\n","4. Loop Untuk Iterasi\n","\n","Loop ini akan melakukan iterasi melalui setiap elemen berita yang ditemukan. Mengambil elemen judul, link berita, kategori, dan tanggal.\n","5. Mengambil Konten Isi Berita\n","  - Mengirim permintaan GET untuk mengakses konten berita lengkap menggunakan link_berita.\n","  - Memeriksa apakah respons dari permintaan tersebut berhasil (kode status 200 menunjukkan sukses).\n","  - Mem-parsing respons HTML dari konten berita menggunakan BeautifulSoup.\n","  - Mencari elemen paragraf (<p>) yang berisi isi berita menggunakan kelas CSS tertentu.\n","  - kemudian dibuat kondisi apakah elemen berisi isi berita ditemukan, jika tidak maka diatur ke 'N/A'\n","  - Menyimpan data berita yang berhasil diambil (judul, isi berita, tanggal, kategori) ke dalam list data_berita. Data ini nantinya akan digunakan untuk membuat DataFrame atau disimpan dalam file CSV.\n","\n"],"metadata":{"id":"VBeAfmuDe6E3"}},{"cell_type":"code","source":["# Loop untuk menangani pagination\n","for page in range(1, 11):  # Misalnya, kita ambil 10 halaman\n","    url = f'{base_url}?page={page}'\n","    response = requests.get(url, headers=headers)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    # Tunggu 2-5 detik sebelum permintaan berikutnya\n","    time.sleep(3)\n","\n","    berita_items = soup.find_all('div', class_='block clearfix text-grey-60 kui-PGc kui-2Qi kui-Ysv')\n","\n","    for item in berita_items:\n","        try:\n","            judul = item.find('h4', class_='hover:underline font-sans leading-tight text-grey-60 hover:underline').get_text(strip=True)\n","            link_berita = 'https://www.kompas.id' + item.find('a')['href']\n","            kategori = item.find('span', text=True).get_text(strip=True) if item.find('span', text=True) else 'N/A'\n","            tanggal = item.find('time')['datetime'] if item.find('time') else 'N/A'\n","\n","            response_berita = requests.get(link_berita, headers=headers)\n","\n","            if response_berita.status_code == 200:\n","                soup_berita = BeautifulSoup(response_berita.text, 'html.parser')\n","                isi_berita_element = soup_berita.find('p', class_='ksm-1ST ksm-2Uv')\n","                if isi_berita_element:\n","                    isi_berita = isi_berita_element.get_text(strip=True)\n","                else:\n","                    isi_berita = 'N/A'\n","            else:\n","                print(f\"Failed to fetch article content, status code: {response_berita.status_code}\")\n","                isi_berita = 'N/A'\n","\n","            data_berita.append({\n","                'Judul': judul,\n","                'Isi Berita': isi_berita,\n","                'Tanggal': tanggal,\n","                'Kategori': kategori,\n","                # 'Link': link_berita\n","            })\n","        except Exception as e:\n","            print(f'Error processing item: {e}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQjps02_bKBI","executionInfo":{"status":"ok","timestamp":1725459990341,"user_tz":-420,"elapsed":53856,"user":{"displayName":"Firdatul A'yuni","userId":"04335010483596107898"}},"outputId":"dd60bb91-529a-4f58-f03e-4889e221fb88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-22-279edfa05864>:16: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n","  kategori = item.find('span', text=True).get_text(strip=True) if item.find('span', text=True) else 'N/A'\n"]}]},{"cell_type":"markdown","source":["Print data berita dengan dataframe"],"metadata":{"id":"AD_t6Ig9iL1d"}},{"cell_type":"code","source":["df = pd.DataFrame(data_berita)\n","print(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XgCZEv3ibNr2","executionInfo":{"status":"ok","timestamp":1725459996466,"user_tz":-420,"elapsed":418,"user":{"displayName":"Firdatul A'yuni","userId":"04335010483596107898"}},"outputId":"b2348110-773f-4490-bd6f-41e04f2ddcf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                Judul  \\\n","0                Nyanyian Kelezatan Kuliner Nusantara   \n","1                                      Kue Basah Asin   \n","2   Icip-icip Soto Betawi dan Mi Ayam di Sekitar S...   \n","3           Musim Semi Kuliner di Sekitar Stasiun MRT   \n","4        Berburu Kuliner Viral di Sekitar Stasiun MRT   \n","..                                                ...   \n","95                           Inspirasi Menu Sederhana   \n","96                    Gurih Legit Bebek Madura Ma’Isa   \n","97      Merayakan Kebinekaan Menu Zamrud Khatulistiwa   \n","98              Cita Rasa Malaysia di Selatan Jakarta   \n","99                 Australia Gencarkan Promosi Pangan   \n","\n","                                           Isi Berita              Tanggal  \\\n","0   Merayakan hari baik dengan makan-makan bermenu...  2024-08-18 14:05:23   \n","1   Selama ini kue basah digambarkan sebagai kue y...  2024-08-18 09:00:38   \n","2   Usahakulinerdi sekitar stasiunMRTsemakin berag...  2024-08-16 14:30:00   \n","3   Setidaknya satu-dua tahun terakhir, kawasan se...  2024-08-11 08:30:09   \n","4                                                 N/A  2024-08-11 08:08:09   \n","..                                                ...                  ...   \n","95                                                N/A  2024-08-04 09:33:41   \n","96  Berbekal ingatan akan leluhurnya,Aisa(67) berd...  2024-08-04 07:00:05   \n","97  Aneka hidangan terbaru disodorkan olehKaum. Ta...  2024-07-25 11:00:55   \n","98  Saat pertama kali melangkah masuk ke dalam ked...  2024-07-13 20:39:19   \n","99  Komisi Perdagangan dan Investasi Australia (Au...  2024-06-21 09:05:06   \n","\n","   Kategori  \n","0   Kuliner  \n","1   Kuliner  \n","2   Kuliner  \n","3   Kuliner  \n","4   Kuliner  \n","..      ...  \n","95  Kuliner  \n","96  Kuliner  \n","97  Kuliner  \n","98  Kuliner  \n","99  Kuliner  \n","\n","[100 rows x 4 columns]\n"]}]},{"cell_type":"markdown","source":["**Menyimpan Data Dalam CSV**"],"metadata":{"id":"Ek-h-AwLiVJv"}},{"cell_type":"code","source":["# Simpan hasil ke CSV jika perlu\n","df.to_csv('/content/drive/MyDrive/KULIAH/SEMESTER 7/PPW/TUGAS/data_berita_kuliner.csv', index=False)"],"metadata":{"id":"d3IzLX7TVPCl"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1UJVTBBcsAIiyXKI-Za03xEBElMLv2xaS","authorship_tag":"ABX9TyNhpzWW9XNZirgP6qTFraSq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}